{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B715141_이익범.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMA+jB4WIJ5WLLsCuTr+AkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Happy-Virus-IkBeom/IT-/blob/master/B715141_%EC%9D%B4%EC%9D%B5%EB%B2%94_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLEmn0WLMEhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun May 24 18:41:23 2020\n",
        "\n",
        "@author: 이익범\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"기존 NN  training\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# The file path to save the data\n",
        "save_file = './model.ckpt'\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "print('Learning Finished!')\n",
        "\n",
        "saver.save(sess,save_file)\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(mnist.test.images[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdmVJ-2mN17I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri May 29 21:01:27 2020\n",
        "\n",
        "@author: 이익범\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"기존 NN test\"\"\"\n",
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# The file path to save the data\n",
        "save_file = './model.ckpt'\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "# weights & bias for nn layers\n",
        "\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "# train my model\n",
        "\n",
        "\n",
        "saver.restore(sess,save_file)\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(mnist.test.images[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxfgSYYQO_Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"CNN mnist training => save parameter\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "print('Learning started. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "\n",
        "saver.save(sess,save_file)\n",
        "\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(mnist.test.images[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbyjxRU1AwMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"CNN Original Test=> restore parameter\"\"\"\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "saver.restore(sess,save_file)\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(mnist.test.images[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADSCIYEiQPR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"CNN_Flipping_Test\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "saver.restore(sess,save_file)\n",
        "\n",
        "X_no_flip = mnist.test.images\n",
        "#X_test = np.rot90(X_no_flip,0)\n",
        "X_test = np.flip(X_no_flip)\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "    X: X_test, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(logits, 1), feed_dict={X: X_test[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(X_test[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obvR8LuHE0xI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Color_Flipped_Test\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "# define cost/loss & optimizer \n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "saver.restore(sess,save_file)\n",
        "\n",
        "flipped = True\n",
        "\n",
        "if flipped:\n",
        "  train_data = 1- mnist.train.images\n",
        "  Color_Flipped_test = 1-  mnist.test.images\n",
        "  print(\"Flipped Mnist\")\n",
        "  \n",
        "else:\n",
        "  print(\"Normal Mnist\")\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: Color_Flipped_test, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(logits, 1), feed_dict={X: Color_Flipped_test[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(Color_Flipped_test[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_JeZLmD98a2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"CNN Rotated Mnist Test\"\"\"\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "saver.restore(sess,save_file)\n",
        "test_load = np.loadtxt('/content/mnist_all_rotation_normalized_float_train_valid.amat')\n",
        "# get train image datas\n",
        "x_rotated_test = test_load[:, :-1] / 1.0\n",
        "\n",
        "# get test image labels\n",
        "y_rotated_label = test_load[:, -1:]\n",
        "y_rotated_label = tf.one_hot(y_rotated_label, depth = 10).eval(session=sess)\n",
        "y_rotated_label = tf.reshape(y_rotated_label,shape=[-1,10]).eval(session=sess)\n",
        "\n",
        "'''\n",
        "# train my model\n",
        "print('Learning stared. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "'''\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "def evaluate(X_sample, y_sample, batch_size=512):\n",
        "    \"\"\"Run a minibatch accuracy op\"\"\"\n",
        "\n",
        "    N = X_sample.shape[0]\n",
        "    correct_sample = 0\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        X_batch = X_sample[i: i + batch_size]\n",
        "        y_batch = y_sample[i: i + batch_size]\n",
        "        N_batch = X_batch.shape[0]\n",
        "\n",
        "        feed = {\n",
        "            X: X_batch,\n",
        "            Y: y_batch,\n",
        "            keep_prob: 1\n",
        "        }\n",
        "\n",
        "        correct_sample += sess.run(accuracy, feed_dict=feed) * N_batch\n",
        "\n",
        "    return correct_sample / N\n",
        "\n",
        "print(\"\\nAccuracy Evaluates\") \n",
        "print(\"-------------------------------\")\n",
        "print('Train Accuracy:', evaluate(x_rotated_train, y_rotated_train_label))\n",
        "print('Test Accuracy:', evaluate(x_rotated_test, y_rotated_label))\n",
        "\n",
        "\n",
        "# Get one and predict\n",
        "print(\"\\nGet one and predict\")\n",
        "print(\"-------------------------------\")\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(y_rotated_label[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), {X: x_rotated_test[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(x_rotated_test[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcDyZUPfysYF",
        "colab_type": "code",
        "outputId": "3d57090a-85b0-429a-f4cd-d2f09336d854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        }
      },
      "source": [
        "\"\"\" Low Memory CNN Training\"\"\"\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "saver.save(sess,save_file)\n",
        "\n",
        "\n",
        "train_load = np.loadtxt('/content/mnist_all_rotation_normalized_float_train_valid.amat')\n",
        "# get train image datas\n",
        "x_rotated_train = train_load[:, :-1] / 1.0\n",
        "\n",
        "# get test image labels\n",
        "y_rotated_train_label = train_load[:, -1:]\n",
        "y_rotated_train_label = tf.one_hot(y_rotated_train_label, depth = 10).eval(session=sess)\n",
        "y_rotated_train_label = tf.reshape(y_rotated_train_label,shape=[-1,10]).eval(session=sess)\n",
        "\n",
        "test_load = np.loadtxt('/content/mnist_all_rotation_normalized_float_test.amat')\n",
        "# get train image datas\n",
        "x_rotated_test = test_load[:, :-1] / 1.0\n",
        "\n",
        "# get test image labels\n",
        "y_rotated_label = test_load[:, -1:]\n",
        "y_rotated_label = tf.one_hot(y_rotated_label, depth = 10).eval(session=sess)\n",
        "y_rotated_label = tf.reshape(y_rotated_label,shape=[-1,10]).eval(session=sess)\n",
        "\n",
        "\n",
        "\n",
        "# train my model\n",
        "print('Learning stared. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(12000 / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = x_rotated_train[i*batch_size:i*batch_size+batch_size], y_rotated_train_label[i*batch_size:i*batch_size+batch_size]\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "def evaluate(X_sample, y_sample, batch_size=512):\n",
        "    \"\"\"Run a minibatch accuracy op\"\"\"\n",
        "\n",
        "    N = X_sample.shape[0]\n",
        "    correct_sample = 0\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        X_batch = X_sample[i: i + batch_size]\n",
        "        y_batch = y_sample[i: i + batch_size]\n",
        "        N_batch = X_batch.shape[0]\n",
        "\n",
        "        feed = {\n",
        "            X: X_batch,\n",
        "            Y: y_batch,\n",
        "            keep_prob: 1\n",
        "        }\n",
        "\n",
        "        correct_sample += sess.run(accuracy, feed_dict=feed) * N_batch\n",
        "\n",
        "    return correct_sample / N\n",
        "\n",
        "print(\"\\nAccuracy Evaluates\") \n",
        "print(\"-------------------------------\")\n",
        "print('Train Accuracy:', evaluate(x_rotated_train, y_rotated_train_label))\n",
        "print('Test Accuracy:', evaluate(x_rotated_test, y_rotated_label))\n",
        "\n",
        "\n",
        "# Get one and predict\n",
        "print(\"\\nGet one and predict\")\n",
        "print(\"-------------------------------\")\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(y_rotated_label[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), {X: x_rotated_test[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(x_rotated_test[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Learning stared. It takes sometime.\n",
            "Epoch: 0001 cost = 2.316545940\n",
            "Epoch: 0002 cost = 1.483605135\n",
            "Epoch: 0003 cost = 0.967549519\n",
            "Epoch: 0004 cost = 0.691942339\n",
            "Epoch: 0005 cost = 0.526397263\n",
            "Epoch: 0006 cost = 0.430066845\n",
            "Epoch: 0007 cost = 0.371523820\n",
            "Epoch: 0008 cost = 0.331157997\n",
            "Epoch: 0009 cost = 0.285715674\n",
            "Epoch: 0010 cost = 0.257411384\n",
            "Epoch: 0011 cost = 0.241253417\n",
            "Epoch: 0012 cost = 0.214239784\n",
            "Epoch: 0013 cost = 0.203717100\n",
            "Epoch: 0014 cost = 0.188326071\n",
            "Epoch: 0015 cost = 0.182159927\n",
            "Learning Finished!\n",
            "\n",
            "Accuracy Evaluates\n",
            "-------------------------------\n",
            "Train Accuracy: 0.9849166665077209\n",
            "Test Accuracy: 0.9445999998474122\n",
            "\n",
            "Get one and predict\n",
            "-------------------------------\n",
            "Label:  [1]\n",
            "Prediction:  [1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1wGQ8ohyt-U",
        "colab_type": "code",
        "outputId": "a5b5e969-8548-4401-b727-f3be38a1cfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        }
      },
      "source": [
        "\"\"\" Low Memory CNN Training, Flipping Training, test\"\"\"\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "saver.save(sess,save_file)\n",
        "\n",
        "train_load = np.loadtxt('/content/mnist_all_rotation_normalized_float_train_valid.amat')\n",
        "# get train image datas\n",
        "x_rotated_train = train_load[:, :-1] / 1.0\n",
        "x_rotated_flipped_train = np.flip(x_rotated_train )\n",
        "# get test image labels\n",
        "y_rotated_train_label = train_load[:, -1:]\n",
        "y_rotated_train_label = tf.one_hot(y_rotated_train_label, depth = 10).eval(session=sess)\n",
        "y_rotated_flipped_train_label = tf.reshape(y_rotated_train_label,shape=[-1,10]).eval(session=sess)\n",
        "y_rotated_flipped_train_label = np.flip(y_rotated_flipped_train_label )\n",
        "test_load = np.loadtxt('/content/mnist_all_rotation_normalized_float_test.amat')\n",
        "# get train image datas\n",
        "x_rotated_test = test_load[:, :-1] / 1.0\n",
        "x_rotated_flipped_test = np.flip(x_rotated_test)\n",
        "# get test image labels\n",
        "y_rotated_label = test_load[:, -1:]\n",
        "y_rotated_label = tf.one_hot(y_rotated_label, depth = 10).eval(session=sess)\n",
        "y_rotated_flipped_test_label = tf.reshape(y_rotated_label,shape=[-1,10]).eval(session=sess)\n",
        "y_rotated_flipped_test_label = np.flip(y_rotated_flipped_test_label)\n",
        "# train my model\n",
        "print('Learning stared. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(12000 / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = x_rotated_flipped_train[i*batch_size:i*batch_size+batch_size], y_rotated_flipped_train_label[i*batch_size:i*batch_size+batch_size]\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "def evaluate(X_sample, y_sample, batch_size=512):\n",
        "    \"\"\"Run a minibatch accuracy op\"\"\"\n",
        "\n",
        "    N = X_sample.shape[0]\n",
        "    correct_sample = 0\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        X_batch = X_sample[i: i + batch_size]\n",
        "        y_batch = y_sample[i: i + batch_size]\n",
        "        N_batch = X_batch.shape[0]\n",
        "\n",
        "        feed = {\n",
        "            X: X_batch,\n",
        "            Y: y_batch,\n",
        "            keep_prob: 1\n",
        "        }\n",
        "\n",
        "        correct_sample += sess.run(accuracy, feed_dict=feed) * N_batch\n",
        "\n",
        "    return correct_sample / N\n",
        "\n",
        "print(\"\\nAccuracy Evaluates\") \n",
        "print(\"-------------------------------\")\n",
        "print('Train Accuracy:', evaluate(x_rotated_flipped_train, y_rotated_flipped_train_label))\n",
        "print('Test Accuracy:', evaluate(x_rotated_flipped_test, y_rotated_flipped_test_label))\n",
        "\n",
        "\n",
        "# Get one and predict\n",
        "print(\"\\nGet one and predict\")\n",
        "print(\"-------------------------------\")\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(y_rotated_flipped_test_label[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), {X: x_rotated_flipped_test[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(x_rotated_flipped_test[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest') \n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Learning stared. It takes sometime.\n",
            "Epoch: 0001 cost = 2.141331415\n",
            "Epoch: 0002 cost = 1.273232954\n",
            "Epoch: 0003 cost = 0.827079914\n",
            "Epoch: 0004 cost = 0.610492926\n",
            "Epoch: 0005 cost = 0.486787662\n",
            "Epoch: 0006 cost = 0.395377245\n",
            "Epoch: 0007 cost = 0.354978230\n",
            "Epoch: 0008 cost = 0.310993304\n",
            "Epoch: 0009 cost = 0.285303826\n",
            "Epoch: 0010 cost = 0.260849673\n",
            "Epoch: 0011 cost = 0.243419752\n",
            "Epoch: 0012 cost = 0.217051770\n",
            "Epoch: 0013 cost = 0.214562067\n",
            "Epoch: 0014 cost = 0.194597153\n",
            "Epoch: 0015 cost = 0.179437068\n",
            "Learning Finished!\n",
            "\n",
            "Accuracy Evaluates\n",
            "-------------------------------\n",
            "Train Accuracy: 0.9859166665077209\n",
            "Test Accuracy: 0.9465400001907348\n",
            "\n",
            "Get one and predict\n",
            "-------------------------------\n",
            "Label:  [8]\n",
            "Prediction:  [8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb5-CiaO5BUI",
        "colab_type": "code",
        "outputId": "e2ffcaaa-8e63-4976-ac58-e555cfba63b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"Rotation (including Flipping) + Scaling Training & Testing Validation Code\"\"\"\n",
        "from __future__ import absolute_import\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "\n",
        "\n",
        "import gzip\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "\n",
        "import csv\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from scipy.misc import imsave\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "\n",
        "URL = 'http://yann.lecun.com/exdb/mnist/'\n",
        "\n",
        "expanded_images = []\n",
        "\n",
        "expanded_labels = []\n",
        "\n",
        "DATA_DIRECTORY = \"data\"\n",
        "\n",
        "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
        "\n",
        "\n",
        "def download(filename):\n",
        "\n",
        "    #Check if directory exists\n",
        "\n",
        "    if not tf.gfile.Exists(DATA_DIRECTORY):\n",
        "\n",
        "        tf.gfile.MakeDirs(DATA_DIRECTORY)\n",
        "\n",
        "    filepath = os.path.join(DATA_DIRECTORY, filename)\n",
        "\n",
        "    #Check if file exists, if not download\n",
        "\n",
        "    if not tf.gfile.Exists(filepath):\n",
        "\n",
        "        filepath, _ = urllib.request.urlretrieve(URL + filename, filepath)\n",
        "\n",
        "        with tf.gfile.GFile(filepath) as f:\n",
        "\n",
        "            size = f.size()\n",
        "\n",
        "        print('Successfully downloaded', filename, size, 'bytes.')\n",
        "\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def extract_data(filename, num):\n",
        "\n",
        "    print('Extracting', filename)\n",
        "\n",
        "    #unzip data\n",
        "\n",
        "    with gzip.open(filename) as bytestream:\n",
        "\n",
        "        bytestream.read(16)\n",
        "\n",
        "        buf = bytestream.read(28 * 28 * num * 1)\n",
        "\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "\n",
        "        data = (data - (255 / 2.0)) / 255 #rescaling value to [-0.5,0.5]\n",
        "\n",
        "        data = data.reshape(num, 28, 28, 1) #reshape into tensor\n",
        "\n",
        "        data = np.reshape(data, [num, -1])\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def extract_labels(filename, num):\n",
        "\n",
        "    print('Extracting', filename)\n",
        "\n",
        "    with gzip.open(filename) as bytestream:\n",
        "\n",
        "        bytestream.read(8)\n",
        "\n",
        "        buf = bytestream.read(1 * num)\n",
        "\n",
        "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "\n",
        "        num_labels_data = len(labels)\n",
        "\n",
        "        one_hot_encoding = np.zeros((num_labels_data,10))\n",
        "\n",
        "        one_hot_encoding[np.arange(num_labels_data),labels] = 1\n",
        "\n",
        "        one_hot_encoding = np.reshape(one_hot_encoding, [-1, 10])\n",
        "\n",
        "    return one_hot_encoding\n",
        "\n",
        "\n",
        "\n",
        "def expand_training_data(images, labels):\n",
        "\n",
        "\n",
        "\n",
        "    global expanded_images\n",
        "\n",
        "    global expanded_labels\n",
        "\n",
        "    directory = os.path.dirname(\"data/New\")\n",
        "\n",
        "    if not tf.gfile.Exists(\"data/New\"):\n",
        "\n",
        "        tf.gfile.MakeDirs(\"data/New\")\n",
        "\n",
        "    k = 0 # counter\n",
        "\n",
        "    for x, y in zip(images, labels):\n",
        "\n",
        "        k = k+1\n",
        "\n",
        "        if k%100==0:\n",
        "\n",
        "            print ('expanding data : %03d / %03d' % (k,np.size(images,0)))\n",
        "\n",
        "\n",
        "\n",
        "        # register original data\n",
        "\n",
        "        expanded_images.append(x)\n",
        "\n",
        "        expanded_labels.append(y)\n",
        "\n",
        "\n",
        "\n",
        "        bg_value = -0.5 # this is regarded as background's value black\n",
        "\n",
        "\n",
        "\n",
        "        image = np.reshape(x, (-1, 28))\n",
        "\n",
        "        for i in range(4):\n",
        "\n",
        "            # rotate the image with random degree\n",
        "\n",
        "            angle = np.random.randint(-90,90,1)\n",
        "\n",
        "            new_img = ndimage.rotate(image,angle,reshape=False, cval=bg_value)\n",
        "\n",
        "\n",
        "\n",
        "            # shift the image with random distance\n",
        "\n",
        "            shift = np.random.randint(-2, 2, 2)\n",
        "\n",
        "            new_img_ = ndimage.shift(new_img,shift, cval=bg_value)\n",
        "\n",
        "\n",
        "\n",
        "            #code for saving some of these for visualization purpose only\n",
        "\n",
        "            image1 = (image*255) + (255 / 2.0)\n",
        "\n",
        "            new_img1 = (new_img_*255) + (255 / 2.0)\n",
        "\n",
        "            # register new training data\n",
        "\n",
        "\n",
        "\n",
        "            expanded_images.append(np.reshape(new_img_, 784))\n",
        "\n",
        "            expanded_labels.append(y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # images and labels are concatenated for random-shuffle at each epoch\n",
        "\n",
        "    # notice that pair of image and label should not be broken\n",
        "\n",
        "    expanded_train_total_data = np.concatenate((expanded_images, expanded_labels), axis=1)\n",
        "\n",
        "    np.random.shuffle(expanded_train_total_data)\n",
        "\n",
        "\n",
        "\n",
        "    return expanded_train_total_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_MNIST_data(use_data_augmentation=True):\n",
        "\n",
        "    # Get the data.\n",
        "\n",
        "    train_data_filename = download('train-images-idx3-ubyte.gz')\n",
        "\n",
        "    train_labels_filename = download('train-labels-idx1-ubyte.gz')\n",
        "\n",
        "    test_data_filename = download('t10k-images-idx3-ubyte.gz')\n",
        "\n",
        "    test_labels_filename = download('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "\n",
        "\n",
        "    # Extract it into numpy arrays.\n",
        "\n",
        "    train_data = extract_data(train_data_filename, 60000)\n",
        "\n",
        "    train_labels = extract_labels(train_labels_filename, 60000)\n",
        "\n",
        "    test_data = extract_data(test_data_filename, 10000)\n",
        "\n",
        "    test_labels = extract_labels(test_labels_filename, 10000)\n",
        "\n",
        "    # Generate a validation set.\n",
        "\n",
        "    print(train_data.shape)\n",
        "\n",
        "    validation_data = train_data[:VALIDATION_SIZE, :]\n",
        "\n",
        "    validation_labels = train_labels[:VALIDATION_SIZE,:]\n",
        "\n",
        "    train_data = train_data[VALIDATION_SIZE:, :]\n",
        "\n",
        "    train_labels = train_labels[VALIDATION_SIZE:,:]\n",
        "\n",
        "\n",
        "\n",
        "    # Concatenate train_data & train_labels for random shuffle\n",
        "\n",
        "    if use_data_augmentation:\n",
        "\n",
        "        train_total_data = expand_training_data(train_data, train_labels)\n",
        "\n",
        "    else:\n",
        "\n",
        "        train_total_data = np.concatenate((train_data, train_labels), axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    train_size = train_total_data.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "    return train_total_data, train_size, validation_data, validation_labels, test_data, test_labels\n",
        "\n",
        "train_total_data, train_size, validation_data, validation_labels, test_data, test_labels = prepare_MNIST_data(True)\n",
        "\n",
        "X_train = expanded_images\n",
        "Y_Train_label = expanded_labels\n",
        "X_test = test_data\n",
        "Y_test_label = test_labels\n",
        "\n",
        "\n",
        "save_file = './model.ckpt'\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 5\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#saver.save(sess,save_file)\n",
        "saver.restore(sess,save_file)\n",
        "\n",
        "\n",
        "# train my model\n",
        "print('Learning stared. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(train_size / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = X_train[i*batch_size:i*batch_size+batch_size], Y_Train_label[i*batch_size:i*batch_size+batch_size]\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "def evaluate(X_sample, y_sample, batch_size=512):\n",
        "    \"\"\"Run a minibatch accuracy op\"\"\"\n",
        "\n",
        "    N = X_sample.shape[0]\n",
        "    correct_sample = 0\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        X_batch = X_sample[i: i + batch_size]\n",
        "        y_batch = y_sample[i: i + batch_size]\n",
        "        N_batch = X_batch.shape[0]\n",
        "\n",
        "        feed = {\n",
        "            X: X_batch,\n",
        "            Y: y_batch,\n",
        "            keep_prob: 1\n",
        "        }\n",
        "\n",
        "        correct_sample += sess.run(accuracy, feed_dict=feed) * N_batch\n",
        "\n",
        "    return correct_sample / N\n",
        "\n",
        "print(\"\\nAccuracy Evaluates\") \n",
        "print(\"-------------------------------\")\n",
        "print('Train Accuracy:', evaluate(X_train, Y_Train_label))\n",
        "print('Test Accuracy:', evaluate(X_test, Y_test_label))\n",
        "\n",
        "\n",
        "# Get one and predict\n",
        "print(\"\\nGet one and predict\")\n",
        "print(\"-------------------------------\")\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(Y_test_label[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), {X: X_test[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "plt.imshow(X_test[r:r + 1].\n",
        "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "plt.show()\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "(60000, 784)\n",
            "expanding data : 100 / 55000\n",
            "expanding data : 200 / 55000\n",
            "expanding data : 300 / 55000\n",
            "expanding data : 400 / 55000\n",
            "expanding data : 500 / 55000\n",
            "expanding data : 600 / 55000\n",
            "expanding data : 700 / 55000\n",
            "expanding data : 800 / 55000\n",
            "expanding data : 900 / 55000\n",
            "expanding data : 1000 / 55000\n",
            "expanding data : 1100 / 55000\n",
            "expanding data : 1200 / 55000\n",
            "expanding data : 1300 / 55000\n",
            "expanding data : 1400 / 55000\n",
            "expanding data : 1500 / 55000\n",
            "expanding data : 1600 / 55000\n",
            "expanding data : 1700 / 55000\n",
            "expanding data : 1800 / 55000\n",
            "expanding data : 1900 / 55000\n",
            "expanding data : 2000 / 55000\n",
            "expanding data : 2100 / 55000\n",
            "expanding data : 2200 / 55000\n",
            "expanding data : 2300 / 55000\n",
            "expanding data : 2400 / 55000\n",
            "expanding data : 2500 / 55000\n",
            "expanding data : 2600 / 55000\n",
            "expanding data : 2700 / 55000\n",
            "expanding data : 2800 / 55000\n",
            "expanding data : 2900 / 55000\n",
            "expanding data : 3000 / 55000\n",
            "expanding data : 3100 / 55000\n",
            "expanding data : 3200 / 55000\n",
            "expanding data : 3300 / 55000\n",
            "expanding data : 3400 / 55000\n",
            "expanding data : 3500 / 55000\n",
            "expanding data : 3600 / 55000\n",
            "expanding data : 3700 / 55000\n",
            "expanding data : 3800 / 55000\n",
            "expanding data : 3900 / 55000\n",
            "expanding data : 4000 / 55000\n",
            "expanding data : 4100 / 55000\n",
            "expanding data : 4200 / 55000\n",
            "expanding data : 4300 / 55000\n",
            "expanding data : 4400 / 55000\n",
            "expanding data : 4500 / 55000\n",
            "expanding data : 4600 / 55000\n",
            "expanding data : 4700 / 55000\n",
            "expanding data : 4800 / 55000\n",
            "expanding data : 4900 / 55000\n",
            "expanding data : 5000 / 55000\n",
            "expanding data : 5100 / 55000\n",
            "expanding data : 5200 / 55000\n",
            "expanding data : 5300 / 55000\n",
            "expanding data : 5400 / 55000\n",
            "expanding data : 5500 / 55000\n",
            "expanding data : 5600 / 55000\n",
            "expanding data : 5700 / 55000\n",
            "expanding data : 5800 / 55000\n",
            "expanding data : 5900 / 55000\n",
            "expanding data : 6000 / 55000\n",
            "expanding data : 6100 / 55000\n",
            "expanding data : 6200 / 55000\n",
            "expanding data : 6300 / 55000\n",
            "expanding data : 6400 / 55000\n",
            "expanding data : 6500 / 55000\n",
            "expanding data : 6600 / 55000\n",
            "expanding data : 6700 / 55000\n",
            "expanding data : 6800 / 55000\n",
            "expanding data : 6900 / 55000\n",
            "expanding data : 7000 / 55000\n",
            "expanding data : 7100 / 55000\n",
            "expanding data : 7200 / 55000\n",
            "expanding data : 7300 / 55000\n",
            "expanding data : 7400 / 55000\n",
            "expanding data : 7500 / 55000\n",
            "expanding data : 7600 / 55000\n",
            "expanding data : 7700 / 55000\n",
            "expanding data : 7800 / 55000\n",
            "expanding data : 7900 / 55000\n",
            "expanding data : 8000 / 55000\n",
            "expanding data : 8100 / 55000\n",
            "expanding data : 8200 / 55000\n",
            "expanding data : 8300 / 55000\n",
            "expanding data : 8400 / 55000\n",
            "expanding data : 8500 / 55000\n",
            "expanding data : 8600 / 55000\n",
            "expanding data : 8700 / 55000\n",
            "expanding data : 8800 / 55000\n",
            "expanding data : 8900 / 55000\n",
            "expanding data : 9000 / 55000\n",
            "expanding data : 9100 / 55000\n",
            "expanding data : 9200 / 55000\n",
            "expanding data : 9300 / 55000\n",
            "expanding data : 9400 / 55000\n",
            "expanding data : 9500 / 55000\n",
            "expanding data : 9600 / 55000\n",
            "expanding data : 9700 / 55000\n",
            "expanding data : 9800 / 55000\n",
            "expanding data : 9900 / 55000\n",
            "expanding data : 10000 / 55000\n",
            "expanding data : 10100 / 55000\n",
            "expanding data : 10200 / 55000\n",
            "expanding data : 10300 / 55000\n",
            "expanding data : 10400 / 55000\n",
            "expanding data : 10500 / 55000\n",
            "expanding data : 10600 / 55000\n",
            "expanding data : 10700 / 55000\n",
            "expanding data : 10800 / 55000\n",
            "expanding data : 10900 / 55000\n",
            "expanding data : 11000 / 55000\n",
            "expanding data : 11100 / 55000\n",
            "expanding data : 11200 / 55000\n",
            "expanding data : 11300 / 55000\n",
            "expanding data : 11400 / 55000\n",
            "expanding data : 11500 / 55000\n",
            "expanding data : 11600 / 55000\n",
            "expanding data : 11700 / 55000\n",
            "expanding data : 11800 / 55000\n",
            "expanding data : 11900 / 55000\n",
            "expanding data : 12000 / 55000\n",
            "expanding data : 12100 / 55000\n",
            "expanding data : 12200 / 55000\n",
            "expanding data : 12300 / 55000\n",
            "expanding data : 12400 / 55000\n",
            "expanding data : 12500 / 55000\n",
            "expanding data : 12600 / 55000\n",
            "expanding data : 12700 / 55000\n",
            "expanding data : 12800 / 55000\n",
            "expanding data : 12900 / 55000\n",
            "expanding data : 13000 / 55000\n",
            "expanding data : 13100 / 55000\n",
            "expanding data : 13200 / 55000\n",
            "expanding data : 13300 / 55000\n",
            "expanding data : 13400 / 55000\n",
            "expanding data : 13500 / 55000\n",
            "expanding data : 13600 / 55000\n",
            "expanding data : 13700 / 55000\n",
            "expanding data : 13800 / 55000\n",
            "expanding data : 13900 / 55000\n",
            "expanding data : 14000 / 55000\n",
            "expanding data : 14100 / 55000\n",
            "expanding data : 14200 / 55000\n",
            "expanding data : 14300 / 55000\n",
            "expanding data : 14400 / 55000\n",
            "expanding data : 14500 / 55000\n",
            "expanding data : 14600 / 55000\n",
            "expanding data : 14700 / 55000\n",
            "expanding data : 14800 / 55000\n",
            "expanding data : 14900 / 55000\n",
            "expanding data : 15000 / 55000\n",
            "expanding data : 15100 / 55000\n",
            "expanding data : 15200 / 55000\n",
            "expanding data : 15300 / 55000\n",
            "expanding data : 15400 / 55000\n",
            "expanding data : 15500 / 55000\n",
            "expanding data : 15600 / 55000\n",
            "expanding data : 15700 / 55000\n",
            "expanding data : 15800 / 55000\n",
            "expanding data : 15900 / 55000\n",
            "expanding data : 16000 / 55000\n",
            "expanding data : 16100 / 55000\n",
            "expanding data : 16200 / 55000\n",
            "expanding data : 16300 / 55000\n",
            "expanding data : 16400 / 55000\n",
            "expanding data : 16500 / 55000\n",
            "expanding data : 16600 / 55000\n",
            "expanding data : 16700 / 55000\n",
            "expanding data : 16800 / 55000\n",
            "expanding data : 16900 / 55000\n",
            "expanding data : 17000 / 55000\n",
            "expanding data : 17100 / 55000\n",
            "expanding data : 17200 / 55000\n",
            "expanding data : 17300 / 55000\n",
            "expanding data : 17400 / 55000\n",
            "expanding data : 17500 / 55000\n",
            "expanding data : 17600 / 55000\n",
            "expanding data : 17700 / 55000\n",
            "expanding data : 17800 / 55000\n",
            "expanding data : 17900 / 55000\n",
            "expanding data : 18000 / 55000\n",
            "expanding data : 18100 / 55000\n",
            "expanding data : 18200 / 55000\n",
            "expanding data : 18300 / 55000\n",
            "expanding data : 18400 / 55000\n",
            "expanding data : 18500 / 55000\n",
            "expanding data : 18600 / 55000\n",
            "expanding data : 18700 / 55000\n",
            "expanding data : 18800 / 55000\n",
            "expanding data : 18900 / 55000\n",
            "expanding data : 19000 / 55000\n",
            "expanding data : 19100 / 55000\n",
            "expanding data : 19200 / 55000\n",
            "expanding data : 19300 / 55000\n",
            "expanding data : 19400 / 55000\n",
            "expanding data : 19500 / 55000\n",
            "expanding data : 19600 / 55000\n",
            "expanding data : 19700 / 55000\n",
            "expanding data : 19800 / 55000\n",
            "expanding data : 19900 / 55000\n",
            "expanding data : 20000 / 55000\n",
            "expanding data : 20100 / 55000\n",
            "expanding data : 20200 / 55000\n",
            "expanding data : 20300 / 55000\n",
            "expanding data : 20400 / 55000\n",
            "expanding data : 20500 / 55000\n",
            "expanding data : 20600 / 55000\n",
            "expanding data : 20700 / 55000\n",
            "expanding data : 20800 / 55000\n",
            "expanding data : 20900 / 55000\n",
            "expanding data : 21000 / 55000\n",
            "expanding data : 21100 / 55000\n",
            "expanding data : 21200 / 55000\n",
            "expanding data : 21300 / 55000\n",
            "expanding data : 21400 / 55000\n",
            "expanding data : 21500 / 55000\n",
            "expanding data : 21600 / 55000\n",
            "expanding data : 21700 / 55000\n",
            "expanding data : 21800 / 55000\n",
            "expanding data : 21900 / 55000\n",
            "expanding data : 22000 / 55000\n",
            "expanding data : 22100 / 55000\n",
            "expanding data : 22200 / 55000\n",
            "expanding data : 22300 / 55000\n",
            "expanding data : 22400 / 55000\n",
            "expanding data : 22500 / 55000\n",
            "expanding data : 22600 / 55000\n",
            "expanding data : 22700 / 55000\n",
            "expanding data : 22800 / 55000\n",
            "expanding data : 22900 / 55000\n",
            "expanding data : 23000 / 55000\n",
            "expanding data : 23100 / 55000\n",
            "expanding data : 23200 / 55000\n",
            "expanding data : 23300 / 55000\n",
            "expanding data : 23400 / 55000\n",
            "expanding data : 23500 / 55000\n",
            "expanding data : 23600 / 55000\n",
            "expanding data : 23700 / 55000\n",
            "expanding data : 23800 / 55000\n",
            "expanding data : 23900 / 55000\n",
            "expanding data : 24000 / 55000\n",
            "expanding data : 24100 / 55000\n",
            "expanding data : 24200 / 55000\n",
            "expanding data : 24300 / 55000\n",
            "expanding data : 24400 / 55000\n",
            "expanding data : 24500 / 55000\n",
            "expanding data : 24600 / 55000\n",
            "expanding data : 24700 / 55000\n",
            "expanding data : 24800 / 55000\n",
            "expanding data : 24900 / 55000\n",
            "expanding data : 25000 / 55000\n",
            "expanding data : 25100 / 55000\n",
            "expanding data : 25200 / 55000\n",
            "expanding data : 25300 / 55000\n",
            "expanding data : 25400 / 55000\n",
            "expanding data : 25500 / 55000\n",
            "expanding data : 25600 / 55000\n",
            "expanding data : 25700 / 55000\n",
            "expanding data : 25800 / 55000\n",
            "expanding data : 25900 / 55000\n",
            "expanding data : 26000 / 55000\n",
            "expanding data : 26100 / 55000\n",
            "expanding data : 26200 / 55000\n",
            "expanding data : 26300 / 55000\n",
            "expanding data : 26400 / 55000\n",
            "expanding data : 26500 / 55000\n",
            "expanding data : 26600 / 55000\n",
            "expanding data : 26700 / 55000\n",
            "expanding data : 26800 / 55000\n",
            "expanding data : 26900 / 55000\n",
            "expanding data : 27000 / 55000\n",
            "expanding data : 27100 / 55000\n",
            "expanding data : 27200 / 55000\n",
            "expanding data : 27300 / 55000\n",
            "expanding data : 27400 / 55000\n",
            "expanding data : 27500 / 55000\n",
            "expanding data : 27600 / 55000\n",
            "expanding data : 27700 / 55000\n",
            "expanding data : 27800 / 55000\n",
            "expanding data : 27900 / 55000\n",
            "expanding data : 28000 / 55000\n",
            "expanding data : 28100 / 55000\n",
            "expanding data : 28200 / 55000\n",
            "expanding data : 28300 / 55000\n",
            "expanding data : 28400 / 55000\n",
            "expanding data : 28500 / 55000\n",
            "expanding data : 28600 / 55000\n",
            "expanding data : 28700 / 55000\n",
            "expanding data : 28800 / 55000\n",
            "expanding data : 28900 / 55000\n",
            "expanding data : 29000 / 55000\n",
            "expanding data : 29100 / 55000\n",
            "expanding data : 29200 / 55000\n",
            "expanding data : 29300 / 55000\n",
            "expanding data : 29400 / 55000\n",
            "expanding data : 29500 / 55000\n",
            "expanding data : 29600 / 55000\n",
            "expanding data : 29700 / 55000\n",
            "expanding data : 29800 / 55000\n",
            "expanding data : 29900 / 55000\n",
            "expanding data : 30000 / 55000\n",
            "expanding data : 30100 / 55000\n",
            "expanding data : 30200 / 55000\n",
            "expanding data : 30300 / 55000\n",
            "expanding data : 30400 / 55000\n",
            "expanding data : 30500 / 55000\n",
            "expanding data : 30600 / 55000\n",
            "expanding data : 30700 / 55000\n",
            "expanding data : 30800 / 55000\n",
            "expanding data : 30900 / 55000\n",
            "expanding data : 31000 / 55000\n",
            "expanding data : 31100 / 55000\n",
            "expanding data : 31200 / 55000\n",
            "expanding data : 31300 / 55000\n",
            "expanding data : 31400 / 55000\n",
            "expanding data : 31500 / 55000\n",
            "expanding data : 31600 / 55000\n",
            "expanding data : 31700 / 55000\n",
            "expanding data : 31800 / 55000\n",
            "expanding data : 31900 / 55000\n",
            "expanding data : 32000 / 55000\n",
            "expanding data : 32100 / 55000\n",
            "expanding data : 32200 / 55000\n",
            "expanding data : 32300 / 55000\n",
            "expanding data : 32400 / 55000\n",
            "expanding data : 32500 / 55000\n",
            "expanding data : 32600 / 55000\n",
            "expanding data : 32700 / 55000\n",
            "expanding data : 32800 / 55000\n",
            "expanding data : 32900 / 55000\n",
            "expanding data : 33000 / 55000\n",
            "expanding data : 33100 / 55000\n",
            "expanding data : 33200 / 55000\n",
            "expanding data : 33300 / 55000\n",
            "expanding data : 33400 / 55000\n",
            "expanding data : 33500 / 55000\n",
            "expanding data : 33600 / 55000\n",
            "expanding data : 33700 / 55000\n",
            "expanding data : 33800 / 55000\n",
            "expanding data : 33900 / 55000\n",
            "expanding data : 34000 / 55000\n",
            "expanding data : 34100 / 55000\n",
            "expanding data : 34200 / 55000\n",
            "expanding data : 34300 / 55000\n",
            "expanding data : 34400 / 55000\n",
            "expanding data : 34500 / 55000\n",
            "expanding data : 34600 / 55000\n",
            "expanding data : 34700 / 55000\n",
            "expanding data : 34800 / 55000\n",
            "expanding data : 34900 / 55000\n",
            "expanding data : 35000 / 55000\n",
            "expanding data : 35100 / 55000\n",
            "expanding data : 35200 / 55000\n",
            "expanding data : 35300 / 55000\n",
            "expanding data : 35400 / 55000\n",
            "expanding data : 35500 / 55000\n",
            "expanding data : 35600 / 55000\n",
            "expanding data : 35700 / 55000\n",
            "expanding data : 35800 / 55000\n",
            "expanding data : 35900 / 55000\n",
            "expanding data : 36000 / 55000\n",
            "expanding data : 36100 / 55000\n",
            "expanding data : 36200 / 55000\n",
            "expanding data : 36300 / 55000\n",
            "expanding data : 36400 / 55000\n",
            "expanding data : 36500 / 55000\n",
            "expanding data : 36600 / 55000\n",
            "expanding data : 36700 / 55000\n",
            "expanding data : 36800 / 55000\n",
            "expanding data : 36900 / 55000\n",
            "expanding data : 37000 / 55000\n",
            "expanding data : 37100 / 55000\n",
            "expanding data : 37200 / 55000\n",
            "expanding data : 37300 / 55000\n",
            "expanding data : 37400 / 55000\n",
            "expanding data : 37500 / 55000\n",
            "expanding data : 37600 / 55000\n",
            "expanding data : 37700 / 55000\n",
            "expanding data : 37800 / 55000\n",
            "expanding data : 37900 / 55000\n",
            "expanding data : 38000 / 55000\n",
            "expanding data : 38100 / 55000\n",
            "expanding data : 38200 / 55000\n",
            "expanding data : 38300 / 55000\n",
            "expanding data : 38400 / 55000\n",
            "expanding data : 38500 / 55000\n",
            "expanding data : 38600 / 55000\n",
            "expanding data : 38700 / 55000\n",
            "expanding data : 38800 / 55000\n",
            "expanding data : 38900 / 55000\n",
            "expanding data : 39000 / 55000\n",
            "expanding data : 39100 / 55000\n",
            "expanding data : 39200 / 55000\n",
            "expanding data : 39300 / 55000\n",
            "expanding data : 39400 / 55000\n",
            "expanding data : 39500 / 55000\n",
            "expanding data : 39600 / 55000\n",
            "expanding data : 39700 / 55000\n",
            "expanding data : 39800 / 55000\n",
            "expanding data : 39900 / 55000\n",
            "expanding data : 40000 / 55000\n",
            "expanding data : 40100 / 55000\n",
            "expanding data : 40200 / 55000\n",
            "expanding data : 40300 / 55000\n",
            "expanding data : 40400 / 55000\n",
            "expanding data : 40500 / 55000\n",
            "expanding data : 40600 / 55000\n",
            "expanding data : 40700 / 55000\n",
            "expanding data : 40800 / 55000\n",
            "expanding data : 40900 / 55000\n",
            "expanding data : 41000 / 55000\n",
            "expanding data : 41100 / 55000\n",
            "expanding data : 41200 / 55000\n",
            "expanding data : 41300 / 55000\n",
            "expanding data : 41400 / 55000\n",
            "expanding data : 41500 / 55000\n",
            "expanding data : 41600 / 55000\n",
            "expanding data : 41700 / 55000\n",
            "expanding data : 41800 / 55000\n",
            "expanding data : 41900 / 55000\n",
            "expanding data : 42000 / 55000\n",
            "expanding data : 42100 / 55000\n",
            "expanding data : 42200 / 55000\n",
            "expanding data : 42300 / 55000\n",
            "expanding data : 42400 / 55000\n",
            "expanding data : 42500 / 55000\n",
            "expanding data : 42600 / 55000\n",
            "expanding data : 42700 / 55000\n",
            "expanding data : 42800 / 55000\n",
            "expanding data : 42900 / 55000\n",
            "expanding data : 43000 / 55000\n",
            "expanding data : 43100 / 55000\n",
            "expanding data : 43200 / 55000\n",
            "expanding data : 43300 / 55000\n",
            "expanding data : 43400 / 55000\n",
            "expanding data : 43500 / 55000\n",
            "expanding data : 43600 / 55000\n",
            "expanding data : 43700 / 55000\n",
            "expanding data : 43800 / 55000\n",
            "expanding data : 43900 / 55000\n",
            "expanding data : 44000 / 55000\n",
            "expanding data : 44100 / 55000\n",
            "expanding data : 44200 / 55000\n",
            "expanding data : 44300 / 55000\n",
            "expanding data : 44400 / 55000\n",
            "expanding data : 44500 / 55000\n",
            "expanding data : 44600 / 55000\n",
            "expanding data : 44700 / 55000\n",
            "expanding data : 44800 / 55000\n",
            "expanding data : 44900 / 55000\n",
            "expanding data : 45000 / 55000\n",
            "expanding data : 45100 / 55000\n",
            "expanding data : 45200 / 55000\n",
            "expanding data : 45300 / 55000\n",
            "expanding data : 45400 / 55000\n",
            "expanding data : 45500 / 55000\n",
            "expanding data : 45600 / 55000\n",
            "expanding data : 45700 / 55000\n",
            "expanding data : 45800 / 55000\n",
            "expanding data : 45900 / 55000\n",
            "expanding data : 46000 / 55000\n",
            "expanding data : 46100 / 55000\n",
            "expanding data : 46200 / 55000\n",
            "expanding data : 46300 / 55000\n",
            "expanding data : 46400 / 55000\n",
            "expanding data : 46500 / 55000\n",
            "expanding data : 46600 / 55000\n",
            "expanding data : 46700 / 55000\n",
            "expanding data : 46800 / 55000\n",
            "expanding data : 46900 / 55000\n",
            "expanding data : 47000 / 55000\n",
            "expanding data : 47100 / 55000\n",
            "expanding data : 47200 / 55000\n",
            "expanding data : 47300 / 55000\n",
            "expanding data : 47400 / 55000\n",
            "expanding data : 47500 / 55000\n",
            "expanding data : 47600 / 55000\n",
            "expanding data : 47700 / 55000\n",
            "expanding data : 47800 / 55000\n",
            "expanding data : 47900 / 55000\n",
            "expanding data : 48000 / 55000\n",
            "expanding data : 48100 / 55000\n",
            "expanding data : 48200 / 55000\n",
            "expanding data : 48300 / 55000\n",
            "expanding data : 48400 / 55000\n",
            "expanding data : 48500 / 55000\n",
            "expanding data : 48600 / 55000\n",
            "expanding data : 48700 / 55000\n",
            "expanding data : 48800 / 55000\n",
            "expanding data : 48900 / 55000\n",
            "expanding data : 49000 / 55000\n",
            "expanding data : 49100 / 55000\n",
            "expanding data : 49200 / 55000\n",
            "expanding data : 49300 / 55000\n",
            "expanding data : 49400 / 55000\n",
            "expanding data : 49500 / 55000\n",
            "expanding data : 49600 / 55000\n",
            "expanding data : 49700 / 55000\n",
            "expanding data : 49800 / 55000\n",
            "expanding data : 49900 / 55000\n",
            "expanding data : 50000 / 55000\n",
            "expanding data : 50100 / 55000\n",
            "expanding data : 50200 / 55000\n",
            "expanding data : 50300 / 55000\n",
            "expanding data : 50400 / 55000\n",
            "expanding data : 50500 / 55000\n",
            "expanding data : 50600 / 55000\n",
            "expanding data : 50700 / 55000\n",
            "expanding data : 50800 / 55000\n",
            "expanding data : 50900 / 55000\n",
            "expanding data : 51000 / 55000\n",
            "expanding data : 51100 / 55000\n",
            "expanding data : 51200 / 55000\n",
            "expanding data : 51300 / 55000\n",
            "expanding data : 51400 / 55000\n",
            "expanding data : 51500 / 55000\n",
            "expanding data : 51600 / 55000\n",
            "expanding data : 51700 / 55000\n",
            "expanding data : 51800 / 55000\n",
            "expanding data : 51900 / 55000\n",
            "expanding data : 52000 / 55000\n",
            "expanding data : 52100 / 55000\n",
            "expanding data : 52200 / 55000\n",
            "expanding data : 52300 / 55000\n",
            "expanding data : 52400 / 55000\n",
            "expanding data : 52500 / 55000\n",
            "expanding data : 52600 / 55000\n",
            "expanding data : 52700 / 55000\n",
            "expanding data : 52800 / 55000\n",
            "expanding data : 52900 / 55000\n",
            "expanding data : 53000 / 55000\n",
            "expanding data : 53100 / 55000\n",
            "expanding data : 53200 / 55000\n",
            "expanding data : 53300 / 55000\n",
            "expanding data : 53400 / 55000\n",
            "expanding data : 53500 / 55000\n",
            "expanding data : 53600 / 55000\n",
            "expanding data : 53700 / 55000\n",
            "expanding data : 53800 / 55000\n",
            "expanding data : 53900 / 55000\n",
            "expanding data : 54000 / 55000\n",
            "expanding data : 54100 / 55000\n",
            "expanding data : 54200 / 55000\n",
            "expanding data : 54300 / 55000\n",
            "expanding data : 54400 / 55000\n",
            "expanding data : 54500 / 55000\n",
            "expanding data : 54600 / 55000\n",
            "expanding data : 54700 / 55000\n",
            "expanding data : 54800 / 55000\n",
            "expanding data : 54900 / 55000\n",
            "expanding data : 55000 / 55000\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Learning stared. It takes sometime.\n",
            "Epoch: 0001 cost = 0.437704556\n",
            "Epoch: 0002 cost = 0.164442056\n",
            "Epoch: 0003 cost = 0.136210457\n",
            "Epoch: 0004 cost = 0.121239011\n",
            "Epoch: 0005 cost = 0.112379891\n",
            "Learning Finished!\n",
            "\n",
            "Accuracy Evaluates\n",
            "-------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-b4407ab015cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAccuracy Evaluates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Train_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-b4407ab015cf>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X_sample, y_sample, batch_size)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;34m\"\"\"Run a minibatch accuracy op\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0mcorrect_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmMt4ebQTVIn",
        "colab_type": "code",
        "outputId": "4267f967-3473-4be0-8232-3879f9a91b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip list "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package                  Version        \n",
            "------------------------ ---------------\n",
            "absl-py                  0.9.0          \n",
            "alabaster                0.7.12         \n",
            "albumentations           0.1.12         \n",
            "altair                   4.1.0          \n",
            "asgiref                  3.2.7          \n",
            "astor                    0.8.1          \n",
            "astropy                  4.0.1.post1    \n",
            "astunparse               1.6.3          \n",
            "atari-py                 0.2.6          \n",
            "atomicwrites             1.4.0          \n",
            "attrs                    19.3.0         \n",
            "audioread                2.1.8          \n",
            "autograd                 1.3            \n",
            "Babel                    2.8.0          \n",
            "backcall                 0.1.0          \n",
            "beautifulsoup4           4.6.3          \n",
            "bleach                   1.5.0          \n",
            "blis                     0.4.1          \n",
            "bokeh                    1.4.0          \n",
            "boto                     2.49.0         \n",
            "boto3                    1.13.13        \n",
            "botocore                 1.16.13        \n",
            "Bottleneck               1.3.2          \n",
            "branca                   0.4.1          \n",
            "bs4                      0.0.1          \n",
            "CacheControl             0.12.6         \n",
            "cachetools               3.1.1          \n",
            "catalogue                1.0.0          \n",
            "certifi                  2020.4.5.1     \n",
            "cffi                     1.14.0         \n",
            "chainer                  6.5.0          \n",
            "chardet                  3.0.4          \n",
            "click                    7.1.2          \n",
            "cloudpickle              1.3.0          \n",
            "cmake                    3.12.0         \n",
            "cmdstanpy                0.4.0          \n",
            "colorlover               0.3.0          \n",
            "community                1.0.0b1        \n",
            "contextlib2              0.5.5          \n",
            "convertdate              2.2.1          \n",
            "coverage                 3.7.1          \n",
            "coveralls                0.5            \n",
            "crcmod                   1.7            \n",
            "cufflinks                0.17.3         \n",
            "cvxopt                   1.2.5          \n",
            "cvxpy                    1.0.31         \n",
            "cycler                   0.10.0         \n",
            "cymem                    2.0.3          \n",
            "Cython                   0.29.18        \n",
            "daft                     0.0.4          \n",
            "dask                     2.12.0         \n",
            "dataclasses              0.7            \n",
            "datascience              0.10.6         \n",
            "decorator                4.4.2          \n",
            "defusedxml               0.6.0          \n",
            "descartes                1.1.0          \n",
            "dill                     0.3.1.1        \n",
            "distributed              1.25.3         \n",
            "Django                   3.0.6          \n",
            "dlib                     19.18.0        \n",
            "docopt                   0.6.2          \n",
            "docutils                 0.15.2         \n",
            "dopamine-rl              1.0.5          \n",
            "earthengine-api          0.1.221        \n",
            "easydict                 1.9            \n",
            "ecos                     2.0.7.post1    \n",
            "editdistance             0.5.3          \n",
            "en-core-web-sm           2.2.5          \n",
            "entrypoints              0.3            \n",
            "ephem                    3.7.7.1        \n",
            "et-xmlfile               1.0.1          \n",
            "fa2                      0.3.5          \n",
            "fancyimpute              0.4.3          \n",
            "fastai                   1.0.61         \n",
            "fastdtw                  0.3.4          \n",
            "fastprogress             0.2.3          \n",
            "fastrlock                0.4            \n",
            "fbprophet                0.6            \n",
            "feather-format           0.4.1          \n",
            "featuretools             0.4.1          \n",
            "filelock                 3.0.12         \n",
            "firebase-admin           4.1.0          \n",
            "fix-yahoo-finance        0.0.22         \n",
            "Flask                    1.1.2          \n",
            "folium                   0.8.3          \n",
            "fsspec                   0.7.4          \n",
            "future                   0.16.0         \n",
            "gast                     0.3.3          \n",
            "GDAL                     2.2.2          \n",
            "gdown                    3.6.4          \n",
            "gensim                   3.6.0          \n",
            "geographiclib            1.50           \n",
            "geopy                    1.17.0         \n",
            "gin-config               0.3.0          \n",
            "glob2                    0.7            \n",
            "google                   2.0.3          \n",
            "google-api-core          1.16.0         \n",
            "google-api-python-client 1.7.12         \n",
            "google-auth              1.7.2          \n",
            "google-auth-httplib2     0.0.3          \n",
            "google-auth-oauthlib     0.4.1          \n",
            "google-cloud-bigquery    1.21.0         \n",
            "google-cloud-core        1.0.3          \n",
            "google-cloud-datastore   1.8.0          \n",
            "google-cloud-firestore   1.7.0          \n",
            "google-cloud-language    1.2.0          \n",
            "google-cloud-storage     1.18.1         \n",
            "google-cloud-translate   1.5.0          \n",
            "google-colab             1.0.0          \n",
            "google-pasta             0.2.0          \n",
            "google-resumable-media   0.4.1          \n",
            "googleapis-common-protos 1.51.0         \n",
            "googledrivedownloader    0.4            \n",
            "graphviz                 0.10.1         \n",
            "grpcio                   1.29.0         \n",
            "gspread                  3.0.1          \n",
            "gspread-dataframe        3.0.6          \n",
            "gym                      0.17.2         \n",
            "h5py                     2.10.0         \n",
            "HeapDict                 1.0.1          \n",
            "holidays                 0.9.12         \n",
            "html5lib                 0.9999999      \n",
            "httpimport               0.5.18         \n",
            "httplib2                 0.17.3         \n",
            "httplib2shim             0.0.3          \n",
            "humanize                 0.5.1          \n",
            "hyperopt                 0.1.2          \n",
            "ideep4py                 2.0.0.post3    \n",
            "idna                     2.9            \n",
            "image                    1.5.31         \n",
            "imageio                  2.4.1          \n",
            "imagesize                1.2.0          \n",
            "imbalanced-learn         0.4.3          \n",
            "imblearn                 0.0            \n",
            "imgaug                   0.2.9          \n",
            "importlib-metadata       1.6.0          \n",
            "imutils                  0.5.3          \n",
            "inflect                  2.1.0          \n",
            "intel-openmp             2020.0.133     \n",
            "intervaltree             2.1.0          \n",
            "ipykernel                4.10.1         \n",
            "ipython                  5.5.0          \n",
            "ipython-genutils         0.2.0          \n",
            "ipython-sql              0.3.9          \n",
            "ipywidgets               7.5.1          \n",
            "itsdangerous             1.1.0          \n",
            "jax                      0.1.67         \n",
            "jaxlib                   0.1.47         \n",
            "jdcal                    1.4.1          \n",
            "jedi                     0.17.0         \n",
            "jieba                    0.42.1         \n",
            "Jinja2                   2.11.2         \n",
            "jmespath                 0.10.0         \n",
            "joblib                   0.15.1         \n",
            "jpeg4py                  0.1.4          \n",
            "jsonschema               2.6.0          \n",
            "jupyter                  1.0.0          \n",
            "jupyter-client           5.3.4          \n",
            "jupyter-console          5.2.0          \n",
            "jupyter-core             4.6.3          \n",
            "kaggle                   1.5.6          \n",
            "kapre                    0.1.3.1        \n",
            "Keras                    2.3.1          \n",
            "Keras-Applications       1.0.8          \n",
            "Keras-Preprocessing      1.1.2          \n",
            "keras-vis                0.4.1          \n",
            "kiwisolver               1.2.0          \n",
            "knnimpute                0.1.0          \n",
            "librosa                  0.6.3          \n",
            "lightgbm                 2.2.3          \n",
            "llvmlite                 0.31.0         \n",
            "lmdb                     0.98           \n",
            "lucid                    0.3.8          \n",
            "LunarCalendar            0.0.9          \n",
            "lxml                     4.2.6          \n",
            "Markdown                 3.2.2          \n",
            "MarkupSafe               1.1.1          \n",
            "matplotlib               3.2.1          \n",
            "matplotlib-venn          0.11.5         \n",
            "missingno                0.4.2          \n",
            "mistune                  0.8.4          \n",
            "mizani                   0.6.0          \n",
            "mkl                      2019.0         \n",
            "mlxtend                  0.14.0         \n",
            "more-itertools           8.3.0          \n",
            "moviepy                  0.2.3.5        \n",
            "mpmath                   1.1.0          \n",
            "msgpack                  1.0.0          \n",
            "multiprocess             0.70.9         \n",
            "multitasking             0.0.9          \n",
            "murmurhash               1.0.2          \n",
            "music21                  5.5.0          \n",
            "natsort                  5.5.0          \n",
            "nbconvert                5.6.1          \n",
            "nbformat                 5.0.6          \n",
            "networkx                 2.4            \n",
            "nibabel                  3.0.2          \n",
            "nltk                     3.2.5          \n",
            "notebook                 5.2.2          \n",
            "np-utils                 0.5.12.1       \n",
            "numba                    0.48.0         \n",
            "numexpr                  2.7.1          \n",
            "numpy                    1.18.4         \n",
            "nvidia-ml-py3            7.352.0        \n",
            "oauth2client             4.1.3          \n",
            "oauthlib                 3.1.0          \n",
            "okgrade                  0.4.3          \n",
            "opencv-contrib-python    4.1.2.30       \n",
            "opencv-python            4.1.2.30       \n",
            "openpyxl                 2.5.9          \n",
            "opt-einsum               3.2.1          \n",
            "osqp                     0.6.1          \n",
            "packaging                20.4           \n",
            "palettable               3.3.0          \n",
            "pandas                   1.0.3          \n",
            "pandas-datareader        0.8.1          \n",
            "pandas-gbq               0.11.0         \n",
            "pandas-profiling         1.4.1          \n",
            "pandocfilters            1.4.2          \n",
            "parso                    0.7.0          \n",
            "pathlib                  1.0.1          \n",
            "patsy                    0.5.1          \n",
            "pexpect                  4.8.0          \n",
            "pickleshare              0.7.5          \n",
            "Pillow                   7.0.0          \n",
            "pip                      19.3.1         \n",
            "pip-tools                4.5.1          \n",
            "plac                     1.1.3          \n",
            "plotly                   4.4.1          \n",
            "plotnine                 0.6.0          \n",
            "pluggy                   0.7.1          \n",
            "portpicker               1.3.1          \n",
            "prefetch-generator       1.0.1          \n",
            "preshed                  3.0.2          \n",
            "prettytable              0.7.2          \n",
            "progressbar2             3.38.0         \n",
            "prometheus-client        0.8.0          \n",
            "promise                  2.3            \n",
            "prompt-toolkit           1.0.18         \n",
            "protobuf                 3.10.0         \n",
            "psutil                   5.4.8          \n",
            "psycopg2                 2.7.6.1        \n",
            "ptvsd                    5.0.0a12       \n",
            "ptyprocess               0.6.0          \n",
            "py                       1.8.1          \n",
            "pyarrow                  0.14.1         \n",
            "pyasn1                   0.4.8          \n",
            "pyasn1-modules           0.2.8          \n",
            "pycocotools              2.0.0          \n",
            "pycparser                2.20           \n",
            "pydata-google-auth       1.1.0          \n",
            "pydot                    1.3.0          \n",
            "pydot-ng                 2.0.0          \n",
            "pydotplus                2.0.2          \n",
            "PyDrive                  1.3.1          \n",
            "pyemd                    0.5.1          \n",
            "pyglet                   1.5.0          \n",
            "Pygments                 2.1.3          \n",
            "pygobject                3.26.1         \n",
            "pymc3                    3.7            \n",
            "PyMeeus                  0.3.7          \n",
            "pymongo                  3.10.1         \n",
            "pymystem3                0.2.0          \n",
            "PyOpenGL                 3.1.5          \n",
            "pyparsing                2.4.7          \n",
            "pyrsistent               0.16.0         \n",
            "pysndfile                1.3.8          \n",
            "PySocks                  1.7.1          \n",
            "pystan                   2.19.1.1       \n",
            "pytest                   3.6.4          \n",
            "python-apt               1.6.5+ubuntu0.2\n",
            "python-chess             0.23.11        \n",
            "python-dateutil          2.8.1          \n",
            "python-louvain           0.14           \n",
            "python-slugify           4.0.0          \n",
            "python-utils             2.4.0          \n",
            "pytz                     2018.9         \n",
            "PyWavelets               1.1.1          \n",
            "PyYAML                   3.13           \n",
            "pyzmq                    19.0.1         \n",
            "qtconsole                4.7.4          \n",
            "QtPy                     1.9.0          \n",
            "regex                    2019.12.20     \n",
            "requests                 2.23.0         \n",
            "requests-oauthlib        1.3.0          \n",
            "resampy                  0.2.2          \n",
            "retrying                 1.3.3          \n",
            "rpy2                     3.2.7          \n",
            "rsa                      4.0            \n",
            "s3fs                     0.4.2          \n",
            "s3transfer               0.3.3          \n",
            "scikit-image             0.16.2         \n",
            "scikit-learn             0.22.2.post1   \n",
            "scipy                    1.2.0          \n",
            "screen-resolution-extra  0.0.0          \n",
            "scs                      2.1.2          \n",
            "seaborn                  0.10.1         \n",
            "Send2Trash               1.5.0          \n",
            "setuptools               47.1.1         \n",
            "setuptools-git           1.2            \n",
            "Shapely                  1.7.0          \n",
            "simplegeneric            0.8.1          \n",
            "six                      1.12.0         \n",
            "sklearn                  0.0            \n",
            "sklearn-pandas           1.8.0          \n",
            "smart-open               2.0.0          \n",
            "snowballstemmer          2.0.0          \n",
            "sortedcontainers         2.1.0          \n",
            "spacy                    2.2.4          \n",
            "Sphinx                   1.8.5          \n",
            "sphinxcontrib-websupport 1.2.2          \n",
            "SQLAlchemy               1.3.17         \n",
            "sqlparse                 0.3.1          \n",
            "srsly                    1.0.2          \n",
            "statsmodels              0.10.2         \n",
            "sympy                    1.1.1          \n",
            "tables                   3.4.4          \n",
            "tabulate                 0.8.7          \n",
            "tbb                      2020.0.133     \n",
            "tblib                    1.6.0          \n",
            "tensorboard              2.2.1          \n",
            "tensorboard-plugin-wit   1.6.0.post3    \n",
            "tensorboardcolab         0.0.22         \n",
            "tensorflow               1.5.0          \n",
            "tensorflow-addons        0.8.3          \n",
            "tensorflow-datasets      2.1.0          \n",
            "tensorflow-estimator     2.2.0          \n",
            "tensorflow-gcs-config    2.1.8          \n",
            "tensorflow-hub           0.8.0          \n",
            "tensorflow-metadata      0.22.0         \n",
            "tensorflow-privacy       0.2.2          \n",
            "tensorflow-probability   0.10.0         \n",
            "tensorflow-tensorboard   1.5.1          \n",
            "termcolor                1.1.0          \n",
            "terminado                0.8.3          \n",
            "testpath                 0.4.4          \n",
            "text-unidecode           1.3            \n",
            "textblob                 0.15.3         \n",
            "textgenrnn               1.4.1          \n",
            "Theano                   1.0.4          \n",
            "thinc                    7.4.0          \n",
            "tifffile                 2020.5.25      \n",
            "toolz                    0.10.0         \n",
            "torch                    1.5.0+cu101    \n",
            "torchsummary             1.5.1          \n",
            "torchtext                0.3.1          \n",
            "torchvision              0.6.0+cu101    \n",
            "tornado                  4.5.3          \n",
            "tqdm                     4.41.1         \n",
            "traitlets                4.3.3          \n",
            "tweepy                   3.6.0          \n",
            "typeguard                2.7.1          \n",
            "typing                   3.6.6          \n",
            "typing-extensions        3.6.6          \n",
            "tzlocal                  1.5.1          \n",
            "umap-learn               0.4.3          \n",
            "uritemplate              3.0.1          \n",
            "urllib3                  1.24.3         \n",
            "vega-datasets            0.8.0          \n",
            "wasabi                   0.6.0          \n",
            "wcwidth                  0.1.9          \n",
            "webencodings             0.5.1          \n",
            "Werkzeug                 1.0.1          \n",
            "wheel                    0.34.2         \n",
            "widgetsnbextension       3.5.1          \n",
            "wordcloud                1.5.0          \n",
            "wrapt                    1.12.1         \n",
            "xarray                   0.15.1         \n",
            "xgboost                  0.90           \n",
            "xkit                     0.0.0          \n",
            "xlrd                     1.1.0          \n",
            "xlwt                     1.3.0          \n",
            "yellowbrick              0.9.1          \n",
            "zict                     2.0.0          \n",
            "zipp                     3.1.0          \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
